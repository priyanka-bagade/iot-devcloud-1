{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "# Saftey Gear Detection Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for latest version\n",
    "<br><div class=danger><b>Important: Before proceeding, please run the following cell to ensure that you are running the most recent version of this sample.</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the current status of this and all documents with ability to update\n",
    "from qarpo.catalog import DemoCatalog\n",
    "import os\n",
    "status = DemoCatalog(os.getcwd(), \"Demo\").ShowRepositoryControls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This sample application demonstrates how a smart video IoT solution may be created using Intel® hardware and software tools to perform safety gear detection.  This solution detects any number of objects within a video frame looking specifically for people, safety vests, and hardhats. \n",
    "\n",
    "The results for each frame are stored in a text file that is later read by a second pass to annotate the input video with boxes around detected objects with a label and probability value.\n",
    "\n",
    "### Key concepts\n",
    "This sample application includes an example for the following:\n",
    "- Application:\n",
    "  - Video and image input is supported using OpenCV\n",
    "  - OpenCV is used to draw bounding boxes around detected objects, labels, and other information\n",
    "  - Visualization of the resulting bounding boxes in the output\n",
    "  - Uses the [Async API](https://docs.openvinotoolkit.org/latest/_docs_IE_DG_Intro_to_Performance.html) feature of the Inference Engine (see [Object Detection Sample](../object-detection-python/object_detection.ipynb) for more details)\n",
    "- Intel® DevCloud for the Edge:\n",
    "  - Submitting inference as jobs that are performed on different edge compute nodes (rather than on the development node hosting this Jupyter* notebook)\n",
    "  - Monitoring job status\n",
    "  - Viewing results and assessing performance for hardware on different compute nodes\n",
    "- [Intel® Distribution of OpenVINO™ toolkit](https://software.intel.com/openvino-toolkit):\n",
    "  - Create the necessary Intermediate Representation (IR) files for the inference model using the [Model Downloader](http://docs.openvinotoolkit.org/latest/_tools_downloader_README.html) and [Model Optimizer](http://docs.openvinotoolkit.org/latest/_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html)\n",
    "  - Run an inference application on multiple hardware devices using the [Inference Engine](http://docs.openvinotoolkit.org/latest/_docs_IE_DG_Deep_Learning_Inference_Engine_DevGuide.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Safety gear detection application\n",
    "The safety gear detection application uses the Intel® Distribution of OpenVINO™ toolkit to perform inference on an input video to locate people, safety vests, and hardhats within each frame.  We will setup, run, and view the results for this application for several different hardware devices (CPU. GPU, etc.) available on the compute nodes within the Intel® DevCloud for the Edge.  To accomplish this, we will be performing the following tasks:\n",
    "\n",
    "1. Use the [Model Optimizer](http://docs.openvinotoolkit.org/latest/_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html) to create the model IR files in the necessary precisions\n",
    "2. Create the job file used to submit running inference on compute nodes\n",
    "3. Submit jobs for different compute nodes and monitor the job status until complete\n",
    "4. View results and assess performance \n",
    "\n",
    "### How it works\n",
    "At startup the safety gear detection application configures itself by parsing the command line arguments and reading the specified labels file.  Once configured, the application loads the specified inference model's IR files into the [Inference Engine](http://docs.openvinotoolkit.org/latest/_docs_IE_DG_Deep_Learning_Inference_Engine_DevGuide.html) and runs the three phases described in the following sections.\n",
    "#### Phase 1:  Pre-processing\n",
    "The frames from the specified input video are read and pre-processed to have the frame data ready for input into inference.  Each frame is resized and its channels are transposed to match the input of the input requirements of the inference model.  The resulting blocks of frame data are written to the binary file `tmp/processed_vid.bin`.\n",
    "\n",
    "#### Phase 2:  Inference\n",
    "The binary file `tmp/processed_vid.bin` is read for input to inference to detect the specific objects: people, safety vests, and hardhats.  For each detected object within a frame, a message is written as a line containing the location, label, etc. to the output text file.\n",
    "\n",
    "#### Phase 3:  Post-processing\n",
    "After the safety gear detection application has finished running inference, a second executable is run to annotate the input video with the results from the output text file.  The final results is an output video with boxes drawn around detected objects with each box appearing with a label and probability value.\n",
    "\n",
    "In this sample, we provide three performance numbers for each architecture. However, they are not hard limits on the solution's performance. It is important to understand that for any application, you may want to combine preprocessing, inference, and post-processing, as opposed to separating them as done here. Combined preprocessing has several advantages:\n",
    "- If inference is run asynchronously on the accelerator, the rest of the system is available for parallel tasks for capturing input, preprocessing future frames, or post-processing past frames.\n",
    "- The inference application pipeline has better data locality, allowing for the reuse of data in caches such as in the application's memory or in the hard drive's cache. \n",
    "\n",
    "To run the application on the Intel® DevCloud for the Edge, a job is submitted to an edge compute node with a hardware accelerator such as Intel® HD Graphics GPU, Intel® Movidius™ Neural Compute Stick 2 and and Intel® Arria® 10 FPGA.  After inference on the input is completed, the output is stored in the appropriate `results/<JobNum>/` directory.  The results are then viewed within this Jupyter* Notebook using the `videoHTML` video playback utility.\n",
    "\n",
    "The application and inference code for this sample is already implemented in the files: \n",
    "- First pass, detection using inference: [`safety_gear_detection.py`](./safety_gear_detection.py)\n",
    "- Second pass, annotation using detection results: [`ROI_writer.cpp`](./ROI_writer.cpp)\n",
    "\n",
    "The following sections will guide you through configuring and running the safety gear detection demo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the IR files for the inference model\n",
    "\n",
    "The Intel® Distribution of OpenVINO™ toolkit includes the [Model Optimizer](http://docs.openvinotoolkit.org/latest/_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html) used to convert and optimize trained models into the Intermediate Representation (IR) model files, and the [Inference Engine](http://docs.openvinotoolkit.org/latest/_docs_IE_DG_Deep_Learning_Inference_Engine_DevGuide.html) that uses the IR model files to run inference on hardware devices.  The IR model files can be created from trained models from popular frameworks (e.g. Caffe\\*, Tensorflow*, etc.).  For this sample, the Caffe* model is supplied in the directory `/data/reference-sample-data/safety-gear-detection/` which contains the model files:\n",
    "- **worker_safety_mobilenet.prototxt** - The deployed [Protocol Buffer](https://developers.google.com/protocol-buffers) file that describes the network architecture to run inference\n",
    "- **worker_safety_mobilenet.caffemodel** - Binary containing trained weights\n",
    "\n",
    "These files will need to be optimized using the Model Optimizer to create the necessary IR files.  We will be running the inference model on different hardware devices which have different requirements on the precision of the model (see [Inference Engine Supported Model Formats](https://docs.openvinotoolkit.org/latest/_docs_IE_DG_supported_plugins_Supported_Devices.html#supported_model_formats) for details).  For our purposes, we will focus on the use of the two most common precisions, FP32 and FP16.\n",
    "\n",
    "For this model, we will run the Model Optimizer using the format:\n",
    "```bash\n",
    "/opt/intel/openvino/deployment_tools/model_optimizer/mo.py \\\n",
    "    --input_model <path_to_caffemodel> \\\n",
    "    --model_name <name> \\\n",
    "    --data_type <data_precision> \\\n",
    "    -o <path_to_output_directory> \n",
    "```\n",
    "\n",
    "The input arguments are as follows:\n",
    "- **--input_model** : The model's input *.caffemodel* file  (the *.prototxt* with the same base name will be automatically found, otherwise the `--input_proto` argument would need to be specified)\n",
    "- **--model_name** : Output model name to use\n",
    "- **--data_type** : The model's data type and precision (e.g. FP16, FP32, INT8, etc.)\n",
    "- **--o** : Output directory where to store the generated IR model files\n",
    "\n",
    "For converting our model, the complete command will look like the following:\n",
    "```bash\n",
    "!/opt/intel/openvino/deployment_tools/model_optimizer/mo.py \\\n",
    "    --input_model /data/reference-sample-data/safety-gear-detection/worker_safety_mobilenet.caffemodel \\\n",
    "    --model_name mobilenet-ssd \\\n",
    "    --data_type <data_precision> \\\n",
    "    -o models/mobilenet-ssd/<data_precision>\n",
    "```\n",
    "We will run the command twice, first with <*data_precision*> set to `FP16` and then `FP32` to get all the IR files we will need to run inference on different devices.\n",
    "\n",
    "<br><div class=note><i><b>Note: </b>More information on how to use Model Optimizer to convert Caffe* models may be found at:[Converting a Caffe* Model](https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_Caffe.html)</i></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to use the Model Optimizer to create the `FP16` and `FP32` model IR files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FP16 IR files\n",
    "!/opt/intel/openvino/deployment_tools/model_optimizer/mo.py \\\n",
    "--input_model /data/reference-sample-data/safety-gear-detection/worker_safety_mobilenet.caffemodel \\\n",
    "--model_name mobilenet-ssd \\\n",
    "--data_type FP16 \\\n",
    "-o models/mobilenet-ssd/FP16 \n",
    "\n",
    "# Create FP32 IR files\n",
    "!/opt/intel/openvino/deployment_tools/model_optimizer/mo.py \\\n",
    "--input_model /data/reference-sample-data/safety-gear-detection/worker_safety_mobilenet.caffemodel \\\n",
    "--model_name mobilenet-ssd \\\n",
    "--data_type FP32 \\\n",
    "-o models/mobilenet-ssd/FP32\n",
    "\n",
    "# find all resulting IR files\n",
    "!echo \"\\nAll IR files that were created:\"\n",
    "!find ./models -name \"*.xml\" -o -name \"*.bin\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><div class=tip><i><b>Tip: </b>The '!' at the beginning of a line is a special Jupyter* Notebook command that allows you to run shell commands as if you are at a command line. The above command will also work in a terminal (with the '!' removed).</i></div>\n",
    "\n",
    "As shown above from the output of the last `!find...` command, the required sets of IR model files (`*.xml` and `*.bin`) have been created.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional exercise: View input without inference\n",
    "\n",
    "If you are curious to see the input video, run the following cell to view the original video stream used for inference and this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qarpo.demoutils import videoHTML\n",
    "!ln -sfn /data data\n",
    "videoHTML('Saftey Gear Detection Video', ['./data/reference-sample-data/safety-gear-detection/Safety_Full_Hat_and_Vest.mp4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the demo\n",
    "\n",
    "Running the next cell will display an interactive user interface allowing you to submit jobs to run the demo on multiple edge compute nodes selected by hardware devices, view the output of each job, and compare performance results across jobs.\n",
    "\n",
    "To run a job:\n",
    "1. Select the desired option in the **Target node** list\n",
    "2. Select the desired device in the **Target architecture** list\n",
    "3. Click the **Submit** button\n",
    "\n",
    "After the **Submit** button is clicked, a tab will appear for the new job with a label in the format \"*status*: *JobID*\".  Once the job status appears as \"Done\", the **Display output** button may be clicked to view the output for the job.\n",
    "\n",
    "After one or more jobs are done, the performance results for each job may be plotted by clicking the **Plot results** button.  Results for each job will be potted as bar graphs for **inference time** and **frames per second**.  Lower values mean better performance for **inference time** and higher values mean better performance for **frames per second**. When comparing results, please keep in mind that some architectures are optimized for highest performance, others for low power or other metrics.\n",
    "\n",
    "Run the next cell to begin the interactive demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import json\n",
    "import qarpo\n",
    "\n",
    "# load job configurations for demo\n",
    "with open('job_config.json') as json_file:  \n",
    "    data = json.load(json_file)\n",
    "    \n",
    "# create and run the user job interface\n",
    "job_interface = qarpo.Interface(data)\n",
    "job_interface.displayUI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "- The complete [Saftey Gear Detection Sample](../../developer_samples/safety-gear-detection-python/safety_gear_detection.ipynb) for this demo\n",
    "- [Jupyter* Notebook Samples](https://devcloud.intel.com/edge/advanced/sample_applications/) - additional sample applications \n",
    "- [Jupyter* Notebook Tutorials](https://devcloud.intel.com/edge/get_started/tutorials) -  tutorials on using and creating Jupyter* Notebooks\n",
    "- [Intel® Distribution of OpenVINO™ toolkit Main Page](https://software.intel.com/openvino-toolkit) - learn more about the tools and use of the Intel® Distribution of OpenVINO™ toolkit for implementing inference on the edge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=background-color:#0071C5;color:white;padding:0.5em;display:table-cell;width:100pc;vertical-align:middle>\n",
    "<img style=float:right src=\"https://devcloud.intel.com/edge/static/images/svg/IDZ_logo.svg\" alt=\"Intel DevCloud logo\" width=\"150px\"/>\n",
    "<a style=color:white>Intel® DevCloud for the Edge</a><br>   \n",
    "<a style=color:white href=\"#top\">Top of Page</a> | \n",
    "<a style=color:white href=\"https://devcloud.intel.com/edge/static/docs/terms/Intel-DevCloud-for-the-Edge-Usage-Agreement.pdf\">Usage Agreement (Intel)</a> | \n",
    "<a style=color:white href=\"https://devcloud.intel.com/edge/static/docs/terms/Colfax_Cloud_Service_Terms_v1.3.pdf\">Service Terms (Colfax)</a>\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Ubuntu)",
   "language": "python",
   "name": "c003-python_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
