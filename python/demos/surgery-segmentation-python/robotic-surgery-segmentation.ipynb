{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "# Robotic Instrument Segmentation Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for latest version\n",
    "<br><div class=danger><b>Important: Before proceeding, please run the following cell to ensure that you are running the most recent version of this sample.</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the current status of this and all documents with ability to update\n",
    "from qarpo.catalog import DemoCatalog\n",
    "import os\n",
    "status = DemoCatalog(os.getcwd(), \"Demo\").ShowRepositoryControls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This sample application demonstrates how a smart video IoT solution may be created using Intel® hardware and software tools to perform robotic instrument segmentation.  This solution performs semantic segmentation to identify the segments of robotic instruments within a video frame.  The identified robotic instruments segments are then highlighted in the output with each segment appearing in a different color.\n",
    "\n",
    "### Key concepts\n",
    "This sample application includes an example for the following:\n",
    "- Application:\n",
    "  - Video and image input is supported using OpenCV\n",
    "  - OpenCV is used to draw bounding boxes around detected objects, labels, and other information\n",
    "  - Visualization of the resulting segmentation in the output\n",
    "- Intel® DevCloud for the Edge:\n",
    "  - Submitting inference as jobs that are performed on different edge compute nodes (rather than on the development node hosting this Jupyter* notebook)\n",
    "  - Monitoring job status\n",
    "  - Viewing results and assessing performance for hardware on different compute nodes\n",
    "- [Intel® Distribution of OpenVINO™ toolkit](https://software.intel.com/openvino-toolkit):\n",
    "  - Create the necessary Intermediate Representation (IR) files for the inference model using [Model Optimizer](http://docs.openvinotoolkit.org/latest/_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html)\n",
    "  - Run an inference application on multiple hardware devices using the [Inference Engine](http://docs.openvinotoolkit.org/latest/_docs_IE_DG_Deep_Learning_Inference_Engine_DevGuide.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application background\n",
    "![Robotic Instrument Challenge](./figures/segmentation.gif)\n",
    "\n",
    "The code in this sample refers to the winning solution by Alexey Shvets, Alexander Rakhlin, Alexandr A. Kalinin, and Vladimir Iglovikov in the [MICCAI 2017 Robotic Instrument Segmentation Challenge](https://endovissub2017-roboticinstrumentsegmentation.grand-challenge.org/). This Jupyter* Notebook has been modified from the original found on [GitHub](https://github.com/ternaus/robot-surgery-segmentation/blob/master/Demo.ipynb) which is made available with an [MIT license](https://github.com/ternaus/robot-surgery-segmentation/blob/master/LICENSE). The data files necessary to run this notebook are included in `/data/robotic-surgery-segmentation`.\n",
    "\n",
    "![TernausNet](./figures/TernausNet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robotic instrument segmentation application\n",
    "The robotic instrument segmentation application uses the Intel® Distribution of OpenVINO™ toolkit to perform inference on an input video to locate robotic instruments within each frame.  We will setup, run, and view the results for this application for several different hardware devices (CPU. GPU, etc.) available on the compute nodes within the Intel® DevCloud for the Edge.  To accomplish this, we will be performing the following tasks:\n",
    "\n",
    "1. Use the [Model Optimizer](http://docs.openvinotoolkit.org/latest/_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html) to create the inference model IR files needed to perform inference\n",
    "2. Create the job file used to submit running inference on compute nodes\n",
    "3. Submit jobs for different compute nodes and monitor the job status until complete\n",
    "4. View results and assess performance \n",
    "\n",
    "### How it works\n",
    "At startup the robotic instrument segmentation application configures itself by parsing the command line arguments.  Once configured, the application loads the specified inference model's IR files into the [Inference Engine](http://docs.openvinotoolkit.org/latest/_docs_IE_DG_Deep_Learning_Inference_Engine_DevGuide.html) and runs inference on the specified input video to identify segments of robotic instruments.  Once identified, each robotic instrument segment is colored to highlight it on the output video.\n",
    "\n",
    "To run the application on the Intel® DevCloud for the Edge, a job is submitted to an edge compute node with a hardware accelerator such as Intel® HD Graphics GPU, Intel® Movidius™ Neural Compute Stick 2 and and Intel® Arria® 10 FPGA.  After inference on the input is completed, the output is stored in the appropriate `results/<JobNum>/` directory.  The results are then viewed within this Jupyter* Notebook using the `videoHTML` video playback utility.\n",
    "\n",
    "The application and inference code for this sample is already implemented in the Python* file [`segmentation_parts.py`](./python/segmentation_parts.py) (and other helper `python/*.py` files).\n",
    "\n",
    "The following sections will guide you through configuring and running the robotic instrument segmentation demo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference with PyTorch (Optional)\n",
    "Run the following cell to perform inference with the PyTorch model on a compute node with an [Intel Core i5-6500TE](https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-) using the code in [pytorch_infer.py](python/pytorch_infer.py). \n",
    "\n",
    "<br><div class=note><i><b>Note: </b>The next cell uses `qsub` to submit a job to be run on a compute node because the login node running this Jupyter* Notebook does not have enough memory to run the task.</i></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the script\n",
    "job_id_infer = !qsub python/run_py.sh -l nodes=1:i5-6500te -F \"python/pytorch_infer.py\" -N pytorch_core\n",
    "if job_id_infer:\n",
    "    print(job_id_infer[0])\n",
    "    progressIndicator('results/', job_id_infer[0]+'.txt', \"Inference\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><div class=danger><b>Wait!: </b>\n",
    "Please wait for the progress bar and job to complete before proceeding to the next cell.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the original input image and the inference results, run the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read, label, and display original input image\n",
    "image = cv2.imread(\"generated/input.png\")\n",
    "plt.figure(1, figsize=(15, 15))\n",
    "plt.subplot(121)\n",
    "plt.axis('off')\n",
    "plt.title(\"Input Image\")\n",
    "plt.imshow(image)\n",
    "\n",
    "# read, lable, and display result image\n",
    "mask = cv2.imread(\"generated/mask.png\")\n",
    "plt.subplot(122)\n",
    "plt.axis('off')\n",
    "plt.title(\"Segmentation\")\n",
    "plt.imshow(mask_overlay(image, mask));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting PyTorch* to ONNX*\n",
    "\n",
    "The ONNX* models need to be generated from the original PyTorch* models to be used with the Intel® Distribution of OpenVINO™ toolkit.  This is done by running [pytorch_to_onnx.py](python/pytorch_to_onnx.py). \n",
    "\n",
    "<br><div class=note><i><b>Note: </b>The next cell uses `qsub` to submit a job to be run on a compute node because the login node running this Jupyter* Notebook does not have enough memory to run the task.</i></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the script\n",
    "job_id_onnx = !qsub python/run_py.sh -l nodes=1:i5-6500te -F \"python/pytorch_to_onnx.py\" -N pytorch_to_onnx\n",
    "if job_id_onnx:\n",
    "    print(job_id_onnx[0])\n",
    "    progressIndicator('results/', job_id_onnx[0]+'.txt', \"Conversion\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><div class=danger><b>Wait!: </b>\n",
    "Please wait for the progress bar and job to complete before proceeding to the next cell.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two ONNX* models that have now be generated are:\n",
    "- **models/onnx/surgical_tools_parts.onnx** - Segmentation model used for identifying robotic instrument segments (\"parts\")\n",
    " - This model will be used to process video and measure performance in later steps \n",
    "- **models/onnx/surgical_tools.onnx** - Segmentation model used for identifying robotic instrument (\"binary\")\n",
    " - This model, along with the `surgical_tools_parts.onnx`, will be used in an extra inference run to show some visualization of the segmentation involved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the IR files for the inference model\n",
    "\n",
    "The Intel® Distribution of OpenVINO™ toolkit includes the [Model Optimizer](http://docs.openvinotoolkit.org/latest/_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html) used to convert and optimize trained models into the Intermediate Representation (IR) model files, and the [Inference Engine](http://docs.openvinotoolkit.org/latest/_docs_IE_DG_Deep_Learning_Inference_Engine_DevGuide.html) that uses the IR model files to run inference on hardware devices.  The IR model files can be created from trained models from popular frameworks (e.g. Caffe\\*, Tensorflow*, etc.). For this sample, the two ONNX* models that will be used were just generated from PyTorch* in the previous step.\n",
    "\n",
    "\n",
    "\n",
    "The `surgical_tools_parts.onnx` and `surgical_tools.onnx` files will need to be optimized using the Model Optimizer to create the necessary IR files.  We will be running the inference model on different hardware devices which have different requirements on the precision of the model (see [Inference Engine Supported Model Formats](https://docs.openvinotoolkit.org/latest/_docs_IE_DG_supported_plugins_Supported_Devices.html#supported_model_formats) for details).  For our purposes, we will focus on the use of the most common precision `FP16` to run on all devices that we target.\n",
    "\n",
    "For this model, we will run the Model Optimizer using the format:\n",
    "```bash\n",
    "/opt/intel/openvino/deployment_tools/model_optimizer/mo.py \\\n",
    "    --input_model <path_to_model> \\\n",
    "    --output_dir <path_to_output_directory> \\\n",
    "    --data_type <data_precision> \\\n",
    "    --move_to_preprocess \\\n",
    "    --scale_values [<scale_values>] \\\n",
    "    --mean_values [<channel_mean_values>]\n",
    "```\n",
    "\n",
    "The input arguments are as follows:\n",
    "- **--input_model** : The model's input *.pb* file\n",
    "- **--output_dir** : Output directory where to store the generated IR model files\n",
    "- **--data_type** : The model's data type and precision (e.g. FP16, FP32, INT8, etc.)\n",
    "- **--move_to_preprocess** : Move mean values to IR preprocess section\n",
    "- **--scale_values** : Scaling (divide by, one per channel) value to apply to input values\n",
    "- **--mean_values** : Mean values (one per channel) to be subtracted from input values before scaling\n",
    "\n",
    "The input shape, scaling values, and mean values we will be using are specific to the model topology being used.  Using the appropriate values for the model that we will use, The complete command will look like the following:\n",
    "```bash\n",
    "!/opt/intel/openvino/deployment_tools/model_optimizer/mo.py \\\n",
    "    --input_model <path_to_model> \\\n",
    "    --data_type <data_precision> \\\n",
    "    --output_dir models/<data_precision> \\\n",
    "    --move_to_preprocess \\\n",
    "    --scale_values [58.395,57.12,57.375] \\\n",
    "    --mean_values [123.75,116.28,103.58]\n",
    "```\n",
    "We will run the command once on each model with <*path_to_model*> set to `models/onnx/surgical_tools.onnx` and `models/onnx/surgical_tools_parts.onnx` and with <*data_precision*> set to `FP16`.  This may be changed (e.g. `FP32`, etc.) as needed to run inference on other different devices.\n",
    "\n",
    "<br><div class=note><i><b>Note: </b>More information on how to use Model Optimizer to convert TensorFlow* models as well as specifying input shape and scaling parameters for common model topologies may be found at:[Converting a ONNX* Model](https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_ONNX.html)\n",
    "</i></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to use the Model Optimizer to create the model IR files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo_onnx.py \\\n",
    "    --input_model \"models/onnx/surgical_tools.onnx\" \\\n",
    "    --output_dir models/ov/FP16/ \\\n",
    "    --data_type FP16 \\\n",
    "    --move_to_preprocess \\\n",
    "    --scale_values \"[0.229, 0.224, 0.225]\" \\\n",
    "    --mean_values \"[0.485, 0.456, 0.406]\"\n",
    "\n",
    "!python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo_onnx.py \\\n",
    "    --input_model \"models/onnx/surgical_tools_parts.onnx\" \\\n",
    "    --output_dir models/ov/FP16/ \\\n",
    "    --data_type FP16 \\\n",
    "    --move_to_preprocess \\\n",
    "    --scale_values \"[0.229, 0.224, 0.225]\" \\\n",
    "    --mean_values \"[0.485, 0.456, 0.406]\"\n",
    "\n",
    "!echo \"\\nAll IR files that were created:\"\n",
    "!find ./models -name \"*.xml\" -o -name \"*.bin\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><div class=tip><i><b>Tip: </b>The '!' at the beginning of a line is a special Jupyter* Notebook command that allows you to run shell commands as if you are at a command line. The above command will also work in a terminal (with the '!' removed).</i></div>\n",
    "\n",
    "As shown above from the output of the last `!find...` command, the required sets of IR model files (`*.xml` and `*.bin`) have been created.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the demo\n",
    "\n",
    "Running the next cell will display an interactive user interface allowing you to submit jobs to run the demo on multiple edge compute nodes selected by hardware devices, view the output of each job, and compare performance results across jobs.\n",
    "\n",
    "To run a job:\n",
    "1. Select the desired option in the **Target node** list\n",
    "2. Select the desired device in the **Target architecture** list\n",
    "3. Click the **Submit** button\n",
    "\n",
    "After the **Submit** button is clicked, a tab will appear for the new job with a label in the format \"*status*: *JobID*\".  Once the job status appears as \"Done\", the **Display output** button may be clicked to view the output for the job.\n",
    "\n",
    "After one or more jobs are done, the performance results for each job may be plotted by clicking the **Plot results** button.  Results for each job will be potted as bar graphs for **inference time** and **frames per second**.  Lower values mean better performance for **inference time** and higher values mean better performance for **frames per second**. When comparing results, please keep in mind that some architectures are optimized for highest performance, others for low power or other metrics.\n",
    "\n",
    "Run the next cell to begin the interactive demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import json\n",
    "import qarpo\n",
    "\n",
    "# load job configurations for demo\n",
    "with open('job_config.json') as json_file:  \n",
    "    data = json.load(json_file)\n",
    "    \n",
    "# create and run the user job interface\n",
    "job_interface = qarpo.Interface(data)\n",
    "job_interface.displayUI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "- The complete [Robotic Instrument Segmentation Sample](../../developer_samples/surgery-segmentation-python/robotic-surgery-segmentation.ipynb) for this demo\n",
    "- [Jupyter* Notebook Samples](https://devcloud.intel.com/edge/advanced/sample_applications/) - additional sample applications \n",
    "- [Jupyter* Notebook Tutorials](https://devcloud.intel.com/edge/get_started/tutorials) -  tutorials on using and creating Jupyter* Notebooks\n",
    "- [Intel® Distribution of OpenVINO™ toolkit Main Page](https://software.intel.com/openvino-toolkit) - learn more about the tools and use of the Intel® Distribution of OpenVINO™ toolkit for implementing inference on the edge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citation\n",
    "    @inproceedings{shvets2018automatic,\n",
    "    title={Automatic Instrument Segmentation in Robot-Assisted Surgery using Deep Learning},\n",
    "    author={Shvets, Alexey A and Rakhlin, Alexander and Kalinin, Alexandr A and Iglovikov, Vladimir I},\n",
    "    booktitle={2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA)},\n",
    "    pages={624--628},\n",
    "    year={2018}\n",
    "    }\n",
    "<br><div class=note><i><b>Note: </b>Citation appears exactly as it appears on source website</i></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=background-color:#0071C5;color:white;padding:0.5em;display:table-cell;width:100pc;vertical-align:middle>\n",
    "<img style=float:right src=\"https://devcloud.intel.com/edge/static/images/svg/IDZ_logo.svg\" alt=\"Intel DevCloud logo\" width=\"150px\"/>\n",
    "<a style=color:white>Intel® DevCloud for the Edge</a><br>   \n",
    "<a style=color:white href=\"#top\">Top of Page</a> | \n",
    "<a style=color:white href=\"https://devcloud.intel.com/edge/static/docs/terms/Intel-DevCloud-for-the-Edge-Usage-Agreement.pdf\">Usage Agreement (Intel)</a> | \n",
    "<a style=color:white href=\"https://devcloud.intel.com/edge/static/docs/terms/Colfax_Cloud_Service_Terms_v1.3.pdf\">Service Terms (Colfax)</a>\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "251.4px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
