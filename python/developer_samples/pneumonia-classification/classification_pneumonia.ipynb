{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "# Pneumonia Classification Sample Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for latest version\n",
    "<br><div class=danger><b>Important: Before proceeding, please run the following cell to ensure that you are running the most recent version of this sample.</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the current status of this and all documents with ability to update\n",
    "from qarpo.catalog import DemoCatalog\n",
    "import os\n",
    "status = DemoCatalog(os.getcwd(), \"Sample\").ShowRepositoryControls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "This sample requires the following:\n",
    "- All files are present and in the following directory structure:\n",
    "    - **classification_pneumonia.ipynb** - This Jupyter* Notebook\n",
    "    - **classification_pneumonia.py** - Python* code for pneumonia classification application\n",
    "    - **model.pb** - TensorFlow trained model\n",
    "    - **utils.py** - Utility Python* code to read and write image results\n",
    "    - **utils_image.py** - Utility Python* code to display results\n",
    "    - **validation_images/** - Test image dataset\n",
    "    - **example_CAM.png**, **example-pneumonia.jpeg** - Images used in this Jupyter* Notebook\n",
    "\n",
    "It is recommended that you have already read the following from [Get Started on the Intel® DevCloud for the Edge](https://devcloud.intel.com/edge/home/):\n",
    "- [Overview of the Intel® DevCloud for the Edge](https://devcloud.intel.com/edge/get_started/devcloud/)\n",
    "- [Overview of the Intel® Distribution of OpenVINO™ toolkit](https://devcloud.intel.com/edge/get_started/openvino/)\n",
    "- [Getting Started Tutorials](https://devcloud.intel.com/edge/get_started/tutorials/)\n",
    "\n",
    "<br><div class=note><i><b>Note: </b>It is assumed that the server this sample is being run on is on the Intel® DevCloud for the Edge which has Jupyter* Notebook customizations and all the required libraries already installed.  If you download or copy to a new server, this sample may not run.</i></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Pillow\n",
    "The Python* source code depends upon the ['Pillow`](https://pypi.org/project/Pillow/) module which is not present by default and must be installed before running this sample.\n",
    "\n",
    "Run the following cell to test for and install `Pillow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Test whether Pillow package is installed\n",
    "    import PIL\n",
    "    print(\"Pillow already installed\")\n",
    "except:\n",
    "    # Exception while importing Pillow, try to install it using pip3\n",
    "    print(\"Installing Pillow\")\n",
    "    !pip3 install Pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This sample application demonstrates how a smart video IoT solution may be created using Intel® hardware and software tools to perform pneumonia classification.  This solution uses an inference model that has been trained to classify the presence of pneumonia using a patient's chest X-ray.  The results are visualized from what the network has learned using the Class Activation Maps (CAM) technique.\n",
    "\n",
    "### Key concepts\n",
    "This sample application includes an example for the following:\n",
    "- Application:\n",
    "  - Image input is supported using OpenCV\n",
    "  - Visualization of the resulting class activation maps\n",
    "- Intel® DevCloud for the Edge:\n",
    "  - Submitting inference as jobs that are performed on different edge compute nodes (rather than on the development node hosting this Jupyter* notebook)\n",
    "  - Monitoring job status\n",
    "  - Viewing results and assessing performance for hardware on different compute nodes\n",
    "- [Intel® Distribution of OpenVINO™ toolkit](https://software.intel.com/openvino-toolkit):\n",
    "  - Create the necessary Intermediate Representation (IR) files for the inference model using the [Model Downloader](http://docs.openvinotoolkit.org/latest/_tools_downloader_README.html) and [Model Optimizer](http://docs.openvinotoolkit.org/latest/_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html)\n",
    "  - Run an inference application on multiple hardware devices using the [Inference Engine](http://docs.openvinotoolkit.org/latest/_docs_IE_DG_Deep_Learning_Inference_Engine_DevGuide.html)\n",
    "  - Access and use of a class activation map as part of inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application background \n",
    "Pneumonia is an inflammatory condition of the lung affecting primarily the small air sacs known as alveoli.\n",
    "Typically symptoms include some combination of cough, chest pain, fever, and trouble breathing [1].\n",
    "Pneumonia affects approximately 450 million people globally (7% of the population) and results in about 4 million deaths per year [2]. To diagnose this disease, chest X-ray images remain the best diagnostic tool.\n",
    "\n",
    "![](example-pneumonia.jpeg) *Chest X-ray image of a patient with Pneumonia*\n",
    "\n",
    "In this application, we use a model trained to classify patients with pneumonia over healthy cases based on their chest X-ray images. The topology used is the DenseNet 121 which is an architecture that has shown to be very efficient at this problem.  DenseNet 121 is the first work to claim a classification rate better than practicing radiologists. The dataset used for training is from the \"Labeled Optical Coherence Tomography (OCT) and Chest X-Ray Images for Classification\" [3] with a [CC BY 4.0 license](https://creativecommons.org/licenses/by/4.0/).\n",
    "The trained model is provided as a frozen Tensorflow model.\n",
    "\n",
    "[1] Ashby B, Turkington C (2007). The encyclopedia of infectious diseases (3rd ed.). New York: Facts on File. p. 242. ISBN 978-0-8160-6397-0. \n",
    "\n",
    "[2] Ruuskanen O, Lahti E, Jennings LC, Murdoch DR (April 2011). \"Viral pneumonia\". Lancet. 377 (9773): 1264–75. \n",
    "\n",
    "[3] [Labeled Optical Coherence Tomography (OCT) and Chest X-Ray Images for Classification](https://data.mendeley.com/datasets/rscbjbr9sj/2) [(Direct link to dataset)](http://dx.doi.org/10.17632/rscbjbr9sj.2#file-41d542e7-7f91-47f6-9ff2-dd8e5a5a7861)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification and class activation maps\n",
    "\n",
    "A Class Activation Map (CAM) [1] is a technique to visualize the regions that are relevant within a Convolutional Neural Network to identify the specific class in the image. \n",
    "The CAM $M(x,y)$ is calculated from the *N* feature maps $f_i(x,y)$ within the last convolutional layer of the network.  The weighted sum of those feature maps based on the weights of the fully connected layer $w_i$, represents how important those feature maps are for the given classification output. \n",
    "\n",
    "$ M(x,y)=\\sum_{i=0}^N w_i f_i(x,y) $\n",
    "\n",
    "When using the Intel® Distribution of OpenVINO™ toolkit, this can be implemented using the Python* code (Found in [classification_pneumonia.py](./classification_pneumonia.py)):\n",
    "\n",
    "```python\n",
    "def class_activation_map_openvino(res, convb, fc, net, fp16):\n",
    "    res_bn = res[convb]\n",
    "    conv_outputs=res_bn[0,:,:,:]\n",
    "    # retrieve layer weights\n",
    "    weights_fc=net.layers.get(fc).blobs[\"weights\"]\n",
    "    # initialize CAM array\n",
    "    cam = np.zeros(dtype=np.float32, shape=conv_outputs.shape[1:])\n",
    "    # perform weighted sum\n",
    "    for i, w in enumerate(weights_fc):\n",
    "        conv_outputs1=conv_outputs[i, :, :]\n",
    "        if fp16:\n",
    "            w=float16_conversion(w)\n",
    "            conv_outputs1=float16_conversion_array(conv_outputs[i, :, :])\n",
    "        cam += w * conv_outputs1\n",
    "    return cam\n",
    "```\n",
    "\n",
    "To calculate the Class Activation Map, we access the output feature maps of the last convolution layer `convb` and the weights of the fully connected layer `fc`. \n",
    "By default, only the output layer is present in the returned output of the network. In order to get the feature maps, the last convolution layer must be added to the output. \n",
    "This is done by using the function [`add_outputs(`*convb*`)`](https://docs.openvinotoolkit.org/latest/ie_python_api/classie__api_1_1IENetwork.html) on the network before loading it to the the device plugin. \n",
    "In order to obtain the layer's name, we recommend [Netron](https://pypi.org/project/netron/), which allows to visualize graphically the model. \n",
    "\n",
    "In our pneumonia classification model, the name of the last convolutional layer `convb` is \"relu_1/Relu\" and the name of the fully-connected layer `fc` is \"predictions_1/MatMul\".\n",
    "\n",
    "\n",
    "In order to get the feature maps, inference must be performed first to get the results. The `res` argument in the code above is the inference output which includes the feature maps by using `add_outputs()`.\n",
    "We access the `fc` layer weights using the call [`net.layers.get(fc).blobs[\"weights\"]`](https://docs.openvinotoolkit.org/latest/ie_python_api/classie__api_1_1IENetLayer.html) on the network `net`.  Before doing the weighted sum, if the FP16 model is being used, the weights and feature maps value must be converted from type `float16`to type `float` using the function `float16_conversion_array()`. The weighted sum of the weights is then done with the feature maps. \n",
    "\n",
    "The result of the function is an image of same size as the feature maps (here, $7\\times7$).  An example is shown below on the left.\n",
    "The CAM image is upsampled to the original input image size and overlaid over the input image. The region of highest value (here, in yellow) will be the region that is the most relevant to deciding on the classification result value. \n",
    "\n",
    "![](example_CAM.png) *CAM image and overlay of CAM with input image*\n",
    "\n",
    "[1] Zhou, Bolei, et al. \"Learning deep features for discriminative localization.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pneumonia classification application\n",
    "The pneumonia classification application uses the Intel® Distribution of OpenVINO™ toolkit to perform inference on an input X-ray image to classify the presence of pneumonia.  We will setup, run, and view the results for this application for several different hardware devices (CPU. GPU, etc.) available on the compute nodes within the Intel® DevCloud for the Edge.  To accomplish this, we will be performing the following tasks:\n",
    "\n",
    "1. Use the [Model Optimizer](http://docs.openvinotoolkit.org/latest/_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html) to create the inference model IR files needed to perform inference\n",
    "2. Create the job file used to submit running inference on compute nodes\n",
    "3. Submit jobs for different compute nodes and monitor the job status until complete\n",
    "4. View results and assess performance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How it works\n",
    "At startup the pneumonia classification application configures itself by parsing the command line arguments.  Once configured, the application loads the specified inference model's IR files into the [Inference Engine](http://docs.openvinotoolkit.org/latest/_docs_IE_DG_Deep_Learning_Inference_Engine_DevGuide.html) and runs inference on the specified input X-ray images.  The result for each input X-ray image is written to the `results.txt` file in the form:\n",
    "   Pneumonia probability: <*% probability*>, Inference performed in <*time*>, Input file: <*input filename*>\n",
    "\n",
    "To run the application on the Intel® DevCloud for the Edge, a job is submitted to an edge compute node with a hardware accelerator such as Intel® HD Graphics GPU, Intel® Movidius™ Neural Compute Stick 2 and and Intel® Arria® 10 FPGA.  After inference on the input is completed, the output is stored in the appropriate `results/<architecture>/` directory.  The results are then viewed within this Jupyter* Notebook using the `show_results` utility.\n",
    "\n",
    "The application and inference code for this sample is already implemented in the Python* files: [`classification_pneumonia.py`](./classification_pneumonia.py), [`utils.py`](./utils.py), and [`utils_image.py`](./utils_image.py).\n",
    "\n",
    "The following sections will guide you through configuring and running the pneumonia classification application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "The following sections describe all the necessary configuration to run the TODO application.\n",
    "#### Command line arguments\n",
    "The application is run from the command line using the following format:\n",
    "```bash\n",
    "python3 classification_pneumonia.py <arguments...>\n",
    "```\n",
    "The required command line _<arguments...>_ to run the Python* executable [`classification_pneumonia.py`](./classification_pneumonia.py) are:\n",
    "- **-m** - Path to the _.xml_ IR file (created using [Model Optimizer](http://docs.openvinotoolkit.org/latest/_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html)) for the inference model. \n",
    "- **-i** - Path to input X-ray image file(s) (may use wildcard characters '?' and '*')\n",
    "- **-o** - The path to where the output video file will be stored\n",
    "- **-d** - Device type to use to run inference (CPU, GPU, MYRIAD, HDDL or HETERO:FPGA,CPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "We begin by importing all the Python* modules that will be used within this Jupyter* Notebook to run and display the results of the pneumonia classification application on the Intel® DevCloud for the Edge:\n",
    "- [os](https://docs.python.org/3/library/os.html#module-os) - Operating system specific module (used for file name parsing)\n",
    "- [time](https://docs.python.org/3/library/time.html#module-time) - Time tracking module (used for measuring execution time)\n",
    "- [matplotlib.pyplot](https://matplotlib.org/) - pyplot is used for displaying output images\n",
    "- [sys](https://docs.python.org/3/library/sys.html#module-sys) - System specific parameters and functions\n",
    "- [qarpo.demoutils](https://github.com/ColfaxResearch/qarpo) - Provides utilities for displaying results and managing jobs from within this Jupyter* Notebook\n",
    "- [utils_image](./utils_image.py) - Provides show_results() utility specific to this sample for displaying the results\n",
    "\n",
    "Run the following cell to import the Python* dependencies needed.\n",
    "\n",
    "<br><div class=tip><b>Tip: </b>Select a cell and then use **Ctrl+Enter** to run that cell.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "from qarpo.demoutils import *\n",
    "from utils_image import show_results\n",
    "print('Imported Python modules successfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the IR files for the inference model\n",
    "\n",
    "The Intel® Distribution of OpenVINO™ toolkit includes the [Model Optimizer](http://docs.openvinotoolkit.org/latest/_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html) used to convert and optimize trained models into the Intermediate Representation (IR) model files, and the [Inference Engine](http://docs.openvinotoolkit.org/latest/_docs_IE_DG_Deep_Learning_Inference_Engine_DevGuide.html) that uses the IR model files to run inference on hardware devices.  The IR model files can be created from trained models from popular frameworks (e.g. Caffe\\*, Tensorflow*, etc.).  For this sample, the model is supplied in the file `model.pb` \n",
    "\n",
    "The `model.pb` file will need to be optimized using the Model Optimizer to create the necessary IR files.  We will be running the inference model on different hardware devices which have different requirements on the precision of the model (see [Inference Engine Supported Model Formats](https://docs.openvinotoolkit.org/latest/_docs_IE_DG_supported_plugins_Supported_Devices.html#supported_model_formats) for details).  For our purposes, we will focus on the use of the two most common precisions, FP32 and FP16.\n",
    "\n",
    "For this model, we will run the Model Optimizer using the format:\n",
    "```bash\n",
    "/opt/intel/openvino/deployment_tools/model_optimizer/mo.py \\\n",
    "    --input_model <path_to_model> \\\n",
    "    --input_shape [<N,H,W,C>]\n",
    "    --data_type <data_precision> \\\n",
    "    -o <path_to_output_directory> \\\n",
    "    --mean_values [<channel_mean_values>] \\\n",
    "    --scale_values [<scale_values>]\n",
    "```\n",
    "\n",
    "The input arguments are as follows:\n",
    "- **--input_model** : The model's input *.pb* file\n",
    "- **--input_shape** : The model's input data shape\n",
    " - **N** = Batch size\n",
    " - **H** = Height\n",
    " - **W** = Width\n",
    " - **C** = Number of channels\n",
    "- **--data_type** : The model's data type and precision (e.g. FP16, FP32, INT8, etc.)\n",
    "- **--o** : Output directory where to store the generated IR model files\n",
    "- **--scale_values** : Scaling (divide by, one per channel) value to apply to input values\n",
    "- **--mean_values** : Mean values (one per channel) to be subtracted from input values before scaling\n",
    "\n",
    "The input shape, scaling values, and mean values we will be using are specific to the model topology being used.  Using the appropriate values for the model that we will use, The complete command will look like the following:\n",
    "```bash\n",
    "!/opt/intel/openvino/deployment_tools/model_optimizer/mo.py \\\n",
    "    --input_model model/crnn.pb \\\n",
    "    --data_type <data_precision> \\\n",
    "    -o models/<data_precision> \\\n",
    "    --mean_values [123.75,116.28,103.58] \\\n",
    "    --scale_values [58.395,57.12,57.375]\n",
    "```\n",
    "We will run the command twice, first with <*data_precision*> set to `FP16` and then `FP32` to get all the IR files we will need to run inference on different devices.\n",
    "\n",
    "<br><div class=note><i><b>Note: </b>More information on how to use Model Optimizer to convert TensorFlow* models as well as specifying input shape and scaling parameters for common model topologies may be found at:[Converting a TensorFlow* Model](https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_TensorFlow.html)\n",
    "</i></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to use the Model Optimizer to create the `FP16` and `FP32` model IR files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FP16 IR files\n",
    "!python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo.py \\\n",
    "--input_model model.pb \\\n",
    "--input_shape=[1,224,224,3] \\\n",
    "--data_type FP16 \\\n",
    "-o models/FP16/ \\\n",
    "--mean_values [123.75,116.28,103.58] \\\n",
    "--scale_values [58.395,57.12,57.375]\n",
    "\n",
    "# Create FP32 IR files\n",
    "!python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo.py \n",
    "--input_model model.pb \\\n",
    "--input_shape=[1,224,224,3] \\\n",
    "--data_type FP32 \\\n",
    "-o models/FP32 \\\n",
    "--mean_values [123.75,116.28,103.58] \\\n",
    "--scale_values [58.395,57.12,57.375] \n",
    "\n",
    "# find all resulting IR files\n",
    "!echo \"\\nAll IR files that were downloaded or created:\"\n",
    "!find ./models -name \"*.xml\" -o -name \"*.bin\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><div class=tip><i><b>Tip: </b>The '!' at the beginning of a line is a special Jupyter* Notebook command that allows you to run shell commands as if you are at a command line. The above command will also work in a terminal (with the '!' removed).</i></div>\n",
    "\n",
    "As shown above from the output of the last `!find...` command, the required sets of IR model files (`*.xml` and `*.bin`) have been created.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run inference\n",
    "The following sections will go through the steps to run our inference application on the Intel® DevCloud for the Edge. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure input video\n",
    "For convenience and consistency, in the next cell we set the Python* variable `InputImages` to the input image file(s) that we will be using to run our sample application.\n",
    "\n",
    "<br><div class=note><i><b>Note: </b>\n",
    "If you want to use different input images, change the path in the following cell to the path of the images and run the cell again.\n",
    "</i></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the input images to use for the rest of this sample\n",
    "InputImages = 'validation_images/PNEUMONIA/*.jpeg'\n",
    "print(f\"Input image files set to:{InputImages}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional exercise: View input images without inference\n",
    "\n",
    "If you are curious to see the input images, run the following cell to view the original video stream used for inference and this sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from IPython.display import Image, display\n",
    "# find all the input images\n",
    "files=glob.glob(InputImages)\n",
    "# display input images\n",
    "for image_file in files:\n",
    "    print(f\"Input image file {image_file}:\")\n",
    "    display(Image(image_file))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the job file\n",
    "We will run inference on several different edge compute nodes present in the Intel® DevCloud for the Edge. We will send work to the edge compute nodes by submitting the corresponding non-interactive jobs into a queue. For each job, we will specify the type of the edge compute server that must be allocated for the job.\n",
    "\n",
    "The job file is a [Bash](https://www.gnu.org/software/bash/) script that serves as a wrapper around the Python* executable of our application that will be executed directly on the edge compute node.  One purpose of the job file is to simplify running an application on different compute nodes by accepting a few arguments and then performing accordingly any necessary steps before and after running the application executable.  \n",
    "\n",
    "For this sample, the job file we will be using is already written for you and appears in the next cell.  The job file will be submitted as if it were run from the command line using the following format:\n",
    "```bash\n",
    "classification_pneumonia_job.sh <output_directory> <device> <fp_precision> <input_file> \n",
    "```\n",
    "Where the job file input arguments are:\n",
    "- <*output_directory*> - Output directory to use to store output files\n",
    "- <*device*> - Hardware device to use (e.g. CPU, GPU, etc.)\n",
    "- <*fp_precision*> - Which floating point precision inference model to use (FP32 or FP16)\n",
    "- <*input_file*> - Path to input image file(s)\n",
    "\n",
    "Based on the input arguments, the job file will do the following:\n",
    "- Change to the working directory `PBS_O_WORKDIR` where this Jupyter* Notebook and other files appear on the compute node\n",
    "- Create the <*output_directory*>\n",
    "- Do additional setup and configuration when running inference on an FPGA hardware device\n",
    "- Choose the appropriate inference model IR file for the specified <*fp_precision*>\n",
    "- Run the application Python* executable with the appropriate command line arguments\n",
    "\n",
    "Run the following cell to create the `classification_pneumonia_job.sh` job file.  The [`%%writefile`](https://ipython.readthedocs.io/en/stable/interactive/magics.html#cellmagic-writefile) line at the top will write the cell contents to the specified job file `classification_pneumonia_job.sh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile classification_pneumonia_job.sh\n",
    "\n",
    "# Store input arguments: <output_directory> <device> <fp_precision> <input_file>\n",
    "OUTPUT_FILE=$1\n",
    "DEVICE=$2\n",
    "FP_MODEL=$3\n",
    "INPUT_FILE=$4\n",
    "\n",
    "# The default path for the job is the user's home directory,\n",
    "#  change directory to where the files are.\n",
    "cd $PBS_O_WORKDIR\n",
    "\n",
    "# Make sure that the output directory exists.\n",
    "mkdir -p $OUTPUT_FILE\n",
    "\n",
    "# Check for special setup steps depending upon device to be used\n",
    "if [ \"$DEVICE\" = \"HETERO:FPGA,CPU\" ]; then\n",
    "    # Environment variables and compilation for edge compute nodes with FPGAs\n",
    "    source /opt/altera/aocl-pro-rte/aclrte-linux64/init_opencl.sh\n",
    "    export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/opt/intel/openvino/bitstreams/a10_vision_design_sg1_bitstreams/BSP/a10_1150_sg1/linux64/lib\n",
    "    aocl program acl0 /opt/intel/openvino/bitstreams/a10_vision_design_sg1_bitstreams/2019R4_PL1_FP11_AlexNet_GoogleNet_Generic.aocx\n",
    "    export CL_CONTEXT_COMPILER_MODE_INTELFPGA=3\n",
    "fi\n",
    "\n",
    "# Set inference model IR files using specified precision\n",
    "MODELPATH=models/${FP_MODEL}/model.xml\n",
    "\n",
    "# Run the pneumonia detection code\n",
    "python3 classification_pneumonia.py -m $MODELPATH \\\n",
    "                                    -i $INPUT_FILE \\\n",
    "                                    -o $OUTPUT_FILE \\\n",
    "                                    -d $DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to submit a job\n",
    "\n",
    "Now that we have the job script, we can submit jobs to edge compute nodes in the Intel® DevCloud for the Edge.  To submit a job, the `qsub` command is used with the following format:\n",
    "```bash\n",
    "qsub <job_file> -N <JobName> -l <nodes> -F \"<job_file_arguments>\" \n",
    "```\n",
    "We can submit classification_pneumonia_job.sh to several different types of edge compute nodes simultaneously or just one node at a time.\n",
    "\n",
    "There are three options of `qsub` command that we use for this:\n",
    "- <*job_file*> - This is the job file we created in the previous step\n",
    "- `-N` <*JobName*> : Sets name specific to the job so that it is easier to distinguish  between it and other jobs\n",
    "- `-l` <*nodes*> - Specifies the number and the type of nodes using the format *nodes*=<*node_count*>:<*property*>[:<*property*>...]\n",
    "- `-F` \"<*job_file_arguments*>\" - String containing the input arguments described in the previous step to use when running the job file\n",
    "\n",
    "*(Optional)*: To see the available types of nodes on the Intel® DevCloud for the Edge, run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pbsnodes | grep compnode | awk '{print $3}' | sort | uniq -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above output from executing the previous cell, the properties describe the node, and the number on the left is the number of available nodes of that architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submit jobs\n",
    "\n",
    "Each of the cells in the subsections below will submit a job to be run on different edge compute nodes. The output of each cell is the _JobID_ for the submitted job.  The _JobID_ can be used to track the status of the job.  After submission, a job will go into a waiting queue before running once the requested compute nodes become available.\n",
    "\n",
    "<br><div class=note><i><b>Note: </b>You may submit all jobs at once or one at a time.</i></div> \n",
    "\n",
    "<br><div class=tip><b>Tip: </b>**Shift+Enter** will run the cell and automatically move you to the next cell. This allows you to use **Shift+Enter** multiple times to quickly run through multiple cells, including markdown cells.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Submit to an edge compute node with an Intel® CPU\n",
    "In the cell below, we submit a job to an edge node with an [Intel® Core™ i5-6500TE](https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz.html) processor. The inference workload will run the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_core = !qsub classification_pneumonia_job.sh -l nodes=1:i5-6500te -F \"results/core CPU FP32 \\\"{InputImages}\\\"\" -N pneum_core\n",
    "print(job_id_core[0])\n",
    "#Progress indicators\n",
    "if job_id_core:\n",
    "    progressIndicator('results/core', 'i_progress_'+job_id_core[0]+'.txt', \"Inference\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Submit to an edge compute node with Intel® Xeon® CPU\n",
    "In the cell below, we submit a job to an edge node with an [Intel® Xeon® Processor E3-1268L v5](https://ark.intel.com/products/88178/Intel-Xeon-Processor-E3-1268L-v5-8M-Cache-2-40-GHz.html). The inference workload will run on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_xeon = !qsub classification_pneumonia_job.sh  -l nodes=1:e3-1268l-v5 -F \"results/xeon/ CPU FP32 \\\"{InputImages}\\\"\" -N pneum_xeon \n",
    "print(job_id_xeon[0])\n",
    "#Progress indicators\n",
    "if job_id_xeon:\n",
    "    progressIndicator('results/xeon', 'i_progress_'+job_id_xeon[0]+'.txt', \"Inference\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Submit to an edge compute node with Intel® Core CPU and using the integrated Intel® GPU\n",
    "In the cell below, we submit a job to an edge node with an [Intel® Core i5-6500TE](https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz.html). The inference workload will run on the Intel® HD Graphics 530 GPU integrated with the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_gpu = !qsub classification_pneumonia_job.sh -l nodes=1:i5-6500te:intel-hd-530 -F \"results/gpu/ GPU FP32 \\\"{InputImages}\\\"\" -N pneum_gpu \n",
    "print(job_id_gpu[0])\n",
    "#Progress indicators\n",
    "if job_id_gpu:\n",
    "    progressIndicator('results/gpu', 'i_progress_'+job_id_gpu[0]+'.txt', \"Inference\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Submit to an edge compute node with Intel® Neural Compute Stick 2\n",
    "In the cell below, we submit a job to an edge node with an [Intel Core i5-6500te](https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz.html) CPU. The inference workload will run on an [Intel® Neural Compute Stick 2](https://software.intel.com/en-us/neural-compute-stick) installed in this node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_ncs2 = !qsub classification_pneumonia_job.sh -l nodes=1:i5-6500te:intel-ncs2 -F \"results/ncs2/ MYRIAD FP16 \\\"{InputImages}\\\"\" -N pneum_ncs2\n",
    "print(job_id_ncs2[0])\n",
    "#Progress indicators\n",
    "if job_id_ncs2:\n",
    "    progressIndicator('results/ncs2/', 'i_progress_'+job_id_ncs2[0]+'.txt', \"Inference\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Submit to an edge compute node with Intel® Atom® and using the integrated Intel® GPU\n",
    "In the cell below, we submit a job to an edge node with an [Intel® Atom® x7-E3950](https://ark.intel.com/products/96488/Intel-Atom-x7-E3950-Processor-2M-Cache-up-to-2-00-GHz.html) processor. The inference workload will run on the integrated Intel® HD Graphics 505 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_atom = !qsub classification_pneumonia_job.sh -l nodes=1:e3950:intel-hd-505 -F \"results/atom/ GPU FP32 \\\"{InputImages}\\\"\" -N pneum_atom\n",
    "print(job_id_atom[0]) \n",
    "#Progress indicators\n",
    "if job_id_atom:\n",
    "    progressIndicator('results/atom/', 'i_progress_'+job_id_atom[0]+'.txt', \"Inference\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Submit to an edge compute node with  Intel® Vision Accelerator Design with Intel® Arria® 10 FPGA\n",
    "In the cell below, we submit a job to an edge node with an [Intel Core™ i5-6500te CPU](https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz.html). The inference workload will run on the [Intel® Vision Accelerator Design with Intel® Arria® 10 FPGA](https://software.intel.com/en-us/openvino-toolkit/hardware#vision-accelerator-design) card installed in this node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_fpga = !qsub classification_pneumonia_job.sh -l nodes=1:i5-6500te:hddl-f -F \"results/fpga/ HETERO:FPGA,CPU FP16 \\\"{InputImages}\\\"\" -N pneum_fpga\n",
    "print(job_id_fpga[0])\n",
    "#Progress indicators\n",
    "if job_id_fpga:\n",
    "    progressIndicator('results/fpga', 'i_progress_'+job_id_fpga[0]+'.txt', \"Inference\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Submit to an edge compute node with Intel® Vision Accelerator Design with Intel® Movidius™ VPUs\n",
    "In the cell below, we submit a job to an edge node with an [Intel Core i5-6500te CPU](https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz.html). The inference workload will run on an [Intel® Vision Accelerator Design with Intel® Movidius™ VPUs](https://software.intel.com/openvino-toolkit/hardware#VPU) accelerator installed in this node that has eight VPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_hddlr = !qsub classification_pneumonia_job.sh -l nodes=1:i5-6500te:hddl-r -F \"results/hddlr/ HDDL FP16 \\\"{InputImages}\\\"\" -N pneum_hddlr\n",
    "print(job_id_hddlr[0])\n",
    "#Progress indicators\n",
    "if job_id_hddlr:\n",
    "    progressIndicator('results/hddlr', 'i_progress_'+job_id_hddlr[0]+'.txt', \"Inference\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor job status\n",
    "\n",
    "To check the status of the jobs that have been submitted, use the `qstat` command.  The custom Jupyter* Notebook widget `liveQstat()` is provided to display the output of `qstat` with live updates.  \n",
    "\n",
    "Run the following cell to display the current job status with periodic updates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liveQstat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the jobs that you have submitted (referenced by the `JobID` that gets displayed right after you submit the jobs in the previous step).\n",
    "There should also be an extra job in the queue named `jupyterhub-singleuser`: this job is your current Jupyter* Notebook session which is always running.\n",
    "\n",
    "The `S` column shows the current status of each job: \n",
    "- If the status is `Q`, then the job is queued and waiting for available resources\n",
    "- If ste status is `R`, then the job is running\n",
    "- If the job is no longer listed, then the job has completed\n",
    "\n",
    "<br><div class=note><i><b>\n",
    "Note: The amount of time spent in the queue depends on the number of users accessing the requested compute nodes. Once the jobs for this sample application begin to run, they should take from 1 to 5 minutes each to complete.\n",
    "</b></i></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><div class=danger><b>Wait!: </b>Please wait for the inference jobs and video rendering to complete before proceeding to the next step to view results.</div>\n",
    "\n",
    "### View results\n",
    "\n",
    "Once the jobs have completed, the queue system outputs the stdout and stderr streams of each job into files with names of the forms <*JobName*>.o<*JobID*> and <*JobName*>.e<*JobID*>, respecitvely.  The *JobName* corresponds to the `-N` option when submitting the job using the `qsub` command.  \n",
    "\n",
    "The output for each job is written to the files `result.txt` and `result_<image_nm>.jpeg` located in the directory `results/<device>` that was specified as the output directory to the job file.  We will now use the utility `show_results()` utility to display the output within this Jupyter* notebook.  Calling `show_results()` from a Python* cell follows the form:\n",
    "```python\n",
    "show_results(result_dir)\n",
    "```\n",
    "The parameters are:\n",
    "- *result_dir* - Path to output directory where the output files `result.txt` and `result_<image_nm>.jpeg` are located\n",
    "\n",
    "Run the cells below to display the results.\n",
    "\n",
    "<br><div class=note><i><b>Note: </b>See [`utils_image.py`](./utils_image.py) for more information on how the results are displayed in Jupyter* Notebooks.</i></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### View results from an Intel® CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('IEI Tank (Intel Core CPU) Results:')\n",
    "show_results('results/core')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### View results from an Intel® Xeon® CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('IEI Tank Xeon (Intel Xeon CPU) Results:')\n",
    "show_results('results/xeon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### View results from an Intel® Core CPU and integrated Intel® GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('IEI Intel GPU (Intel Core + Intel GPU) Results:')\n",
    "show_results('results/gpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### View results from an Intel® Neural Compute Stick 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('IEI Tank + Intel CPU + Intel NCS2 Results:')\n",
    "show_results('results/ncs2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### View results from an Intel® Atom® and integrated Intel® GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Intel Atom CPU + Intel GPU Results:')\n",
    "show_results('results/atom')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### View results from an Intel® Vision Accelerator Design with Intel® Arria® 10 FPGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Intel FPGA HDDL-F Results:')\n",
    "show_results('results/fpga')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### View results from Intel® Vision Accelerator Design with Intel® Movidius™ VPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('Intel VPU HDDL-R Results:')\n",
    "show_results('results/hddlr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View and assess performance results\n",
    "\n",
    "The running time of each inference task is recorded in `results/<device>/stats.txt`. Run the cell below to plot the results of all jobs side-by-side. Lower values mean better performance for **Inference Engine Processing Time** and higher values mean better performance for **Inference Engine FPS**. When comparing results, please keep in mind that some architectures are optimized for highest performance, others for low power or other metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table of <architecture>, <title> for plotting\n",
    "arch_list = [('core', 'Intel Core\\ni5-6500TE\\nCPU'),\n",
    "             ('xeon', 'Intel Xeon\\nE3-1268L v5\\nCPU'),\n",
    "             ('gpu', ' Intel Core\\ni5-6500TE\\nHD530/GPU'),\n",
    "             ('ncs2', 'Intel VPU\\ni5-6500TE\\nNCS2 VPU'),\n",
    "             ('atom', 'Intel Atom\\nx7-E3950\\nHD505/GPU'),\n",
    "             ('fpga', ' Intel FPGA\\nArria 10\\nHDDL-F'),\n",
    "             ('hddlr', 'Intel VPU\\n8xVPU\\nHDDL-R')]\n",
    "# For each archtecture in table, create path to stats file or placeholder \n",
    "stats_list = []\n",
    "for arch, a_name in arch_list:\n",
    "    # if job_id_<architecture> exists, the job was run and has a stats file\n",
    "    if 'job_id_'+arch in vars():\n",
    "        stats_list.append(('results/{arch}/stats.txt'.format(arch=arch), a_name))\n",
    "    else:\n",
    "        stats_list.append(('placeholder'+arch, a_name))\n",
    "# Plot the execution time from the stats files\n",
    "summaryPlot(stats_list, 'Architecture', 'Time, seconds', 'Inference Engine Processing Time', 'time')\n",
    "# Plot the frames per second from the stats files\n",
    "summaryPlot(stats_list, 'Architecture', 'Frames per second', 'Inference Engine FPS', 'fps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "- [More Jupyter* Notebook Samples](https://devcloud.intel.com/edge/advanced/sample_applications/) - additional sample applications \n",
    "- [Jupyter* Notebook Tutorials](https://devcloud.intel.com/edge/get_started/tutorials) - sample application Jupyter* Notebook tutorials\n",
    "- [Intel® Distribution of OpenVINO™ toolkit Main Page](https://software.intel.com/openvino-toolkit) - learn more about the tools and use of the Intel® Distribution of OpenVINO™ toolkit for implementing inference on the edge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this notebook\n",
    "\n",
    "For technical support, please see the [Intel® DevCloud Forums](https://software.intel.com/en-us/forums/intel-devcloud-for-edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=background-color:#0071C5;color:white;padding:0.5em;display:table-cell;width:100pc;vertical-align:middle>\n",
    "<img style=float:right src=\"https://devcloud.intel.com/edge/static/images/svg/IDZ_logo.svg\" alt=\"Intel DevCloud logo\" width=\"150px\"/>\n",
    "<a style=color:white>Intel® DevCloud for the Edge</a><br>   \n",
    "<a style=color:white href=\"#top\">Top of Page</a> | \n",
    "<a style=color:white href=\"https://devcloud.intel.com/edge/static/docs/terms/Intel-DevCloud-for-the-Edge-Usage-Agreement.pdf\">Usage Agreement (Intel)</a> | \n",
    "<a style=color:white href=\"https://devcloud.intel.com/edge/static/docs/terms/Colfax_Cloud_Service_Terms_v1.3.pdf\">Service Terms (Colfax)</a>\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Ubuntu)",
   "language": "python",
   "name": "c003-python_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "251.4px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
